{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "datananme = 'js'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47127338, 93)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data using pd\n",
    "\n",
    "# pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "# the data is stored in the parent dir with dir_name being datananme\n",
    "train_pd = pd.read_parquet(f'../{datananme}/train.parquet/')\n",
    "train_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data using pl\n",
    "\n",
    "train_pl = pl.read_parquet(f'../{datananme}/train.parquet/')\n",
    "train_pl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data checking/visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# missing values for all of the features for rows that 'responder_6'\n",
    "# is not None\n",
    "\n",
    "from matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\n",
    "\n",
    "supervised_usable = (\n",
    "    train_pl\n",
    "    .filter(pl.col('responder_6').is_not_null())\n",
    ")\n",
    "\n",
    "missing_count = (\n",
    "    supervised_usable\n",
    "    .null_count()\n",
    "    .transpose(include_header=True,\n",
    "               header_name='feature',\n",
    "               column_names=['null_count'])\n",
    "    .sort('null_count', descending=True)\n",
    "    .with_columns((pl.col('null_count') / len(supervised_usable)).alias('null_ratio'))\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6, 20))\n",
    "plt.title(f'Missing values over the {len(supervised_usable)} samples which have a target')\n",
    "plt.barh(np.arange(len(missing_count)), missing_count.get_column('null_ratio'), color='coral', label='missing')\n",
    "plt.barh(np.arange(len(missing_count)), \n",
    "         1 - missing_count.get_column('null_ratio'),\n",
    "         left=missing_count.get_column('null_ratio'),\n",
    "         color='darkseagreen', label='available')\n",
    "plt.yticks(np.arange(len(missing_count)), missing_count.get_column('feature'))\n",
    "plt.gca().xaxis.set_major_formatter(PercentFormatter(xmax=1, decimals=0))\n",
    "plt.xlim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check NaNs for 'responder_6'\n",
    "train_pd['responder_6'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# view lags.parquet\n",
    "\n",
    "lags_pd = pd.read_parquet(f'../{datananme}/lags.parquet/date_id=0')\n",
    "lags_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the sample test data\n",
    "\n",
    "test_pd = pd.read_parquet(f'../{datananme}/test.parquet/date_id=0/part-0.parquet')\n",
    "test_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some features\n",
    "\n",
    "col_to_drop = [f'responder_{i}' for i in range(9) if i != 6] \\\n",
    "                + ['partition_id']\n",
    "\n",
    "train_pd1 = train_pd.drop(labels=col_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35370822, 84)\n"
     ]
    }
   ],
   "source": [
    "# drop rows with NAs\n",
    "# but perhaps no need? xgboost shall be able to handle NA by default\n",
    "\n",
    "train_pd2 = train_pd1.dropna(axis=0)\n",
    "print(train_pd2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to predictors and y, since there is no categorical features\n",
    "\n",
    "col_weight = ['weight']\n",
    "col_y = ['responder_6']\n",
    "col_num = [col for col in train_pd2.columns.tolist() if col not in col_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_r2(y_true, y_pred, weights):\n",
    "#     \"\"\"\n",
    "#     Args:\n",
    "#         y_true, y_pred, weights: np.arrays.\n",
    "#     \"\"\"\n",
    "#     numerator = np.sum(weights * (y_true - y_pred) ** 2)\n",
    "#     denominator = np.sum(weights * (y_true ** 2))\n",
    "#     r2_score = 1 - (numerator / denominator)\n",
    "#     return r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------"
     ]
    }
   ],
   "source": [
    "def calculate_r2_batched(y_true, y_pred, weights, batch_size=100000):\n",
    "    \"\"\"\n",
    "    Calculate the weighted R^2 score using small batches.\n",
    "    \n",
    "    Args:\n",
    "        y_true, y_pred, weights: np.arrays.\n",
    "    \"\"\"\n",
    "    total_weighted_error = 0\n",
    "    total_weighted_square_true = 0\n",
    "    num_batches = int(np.ceil(len(y_true) / batch_size))\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = min((i + 1) * batch_size, len(y_true))\n",
    "\n",
    "        y_true_batch = y_true[start:end]\n",
    "        y_pred_batch = y_pred[start:end]\n",
    "        weights_batch = weights[start:end]\n",
    "\n",
    "        numerator_batch = np.sum(weights_batch * (y_true_batch - y_pred_batch) ** 2)\n",
    "        denominator_batch = np.sum(weights_batch * y_true_batch ** 2)\n",
    "\n",
    "        total_weighted_error += numerator_batch\n",
    "        total_weighted_square_true += denominator_batch\n",
    "        \n",
    "        print('-', end='') # TEST\n",
    "\n",
    "    r2_score = 1 - (total_weighted_error / total_weighted_square_true)\n",
    "    return r2_score\n",
    "\n",
    "r2_score = calculate_r2_batched(y_valid.to_numpy(), y_valid_pred, w_valid.to_numpy())\n",
    "r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def time_series_cross_validation(date_ids, n_splits):\n",
    "    fold_size = len(date_ids) // n_splits\n",
    "\n",
    "    for i in range(n_splits):\n",
    "        # Here, the training set includes all data up to i-th fold\n",
    "        end_train = (i + 1) * fold_size\n",
    "        train_dates = date_ids[:end_train]\n",
    "        # Validation dates are the next fold_size data points after the training set\n",
    "        valid_dates = date_ids[end_train:end_train + fold_size] if end_train + fold_size <= len(date_ids) else date_ids[end_train:]\n",
    "        \n",
    "        yield train_dates, valid_dates\n",
    "        \n",
    "\n",
    "# date_ids = train_pd2['date_id'].unique()\n",
    "\n",
    "# n_splits = 5\n",
    "\n",
    "# for train_dates, valid_dates in time_series_cross_validation(date_ids, n_splits):\n",
    "#     print(\"Train Dates:\", train_dates)\n",
    "#     print(\"Valid Dates:\", valid_dates)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:0.80886\tvalid-rmse:0.85211\n",
      "[10]\ttrain-rmse:0.74917\tvalid-rmse:0.91122\n",
      "[20]\ttrain-rmse:0.69859\tvalid-rmse:0.98388\n",
      "[30]\ttrain-rmse:0.66358\tvalid-rmse:1.02927\n",
      "[40]\ttrain-rmse:0.63271\tvalid-rmse:1.06170\n",
      "[50]\ttrain-rmse:0.60450\tvalid-rmse:1.09480\n",
      "[60]\ttrain-rmse:0.57909\tvalid-rmse:1.12086\n",
      "[70]\ttrain-rmse:0.55682\tvalid-rmse:1.13795\n",
      "[80]\ttrain-rmse:0.53793\tvalid-rmse:1.15427\n",
      "[90]\ttrain-rmse:0.52049\tvalid-rmse:1.16452\n",
      "[99]\ttrain-rmse:0.50372\tvalid-rmse:1.18246\n"
     ]
    }
   ],
   "source": [
    "# xgboost\n",
    "# cross-validation on dates\n",
    "import xgboost as xgb\n",
    "\n",
    "## some hyperparameters \n",
    "verbose_eval = 10\n",
    "early_stopping_rounds = 100\n",
    "num_boost_round = 1000\n",
    "\n",
    "## main part\n",
    "date_ids = train_pd2['date_id'].unique()\n",
    "k = 5\n",
    "fold_size = date_ids.shape[0] // k\n",
    "\n",
    "# for i in range(k):\n",
    "#     start = i * fold_size\n",
    "#     end = (i + 1) * fold_size if i != k - 1 else len(date_ids_ls)\n",
    "#     valid_dates = date_ids[start:end]\n",
    "#     train_dates = np.concatenate([date_ids[:start], date_ids[end:]])\n",
    "\n",
    "for train_dates, valid_dates in time_series_cross_validation(date_ids, n_splits):\n",
    "    X_train = train_pd2[col_num].loc[train_pd2['date_id'].isin(train_dates)]\n",
    "    y_train = train_pd2[col_y].loc[train_pd2['date_id'].isin(train_dates)]\n",
    "    w_train = train_pd2[col_weight].loc[train_pd2['date_id'].isin(train_dates)]\n",
    "\n",
    "    X_valid = train_pd2[col_num].loc[train_pd2['date_id'].isin(valid_dates)]\n",
    "    y_valid = train_pd2[col_y].loc[train_pd2['date_id'].isin(valid_dates)]\n",
    "    w_valid = train_pd2[col_weight].loc[train_pd2['date_id'].isin(valid_dates)]\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train, weight=w_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, label=y_valid, weight=w_valid)\n",
    "\n",
    "    XGB_PARAMS = {\n",
    "        'eval_metric': 'rmse',\n",
    "        'learning_rate': 0.5,\n",
    "        'max_depth': 12,\n",
    "        'min_child_weight': 1.5,\n",
    "        'subsample': 0.8555,\n",
    "        'colsample_bytree': 0.85555555,\n",
    "        'random_state': 42,\n",
    "        'tree_method': 'hist',\n",
    "        'device': 'cuda'\n",
    "    }\n",
    "\n",
    "    model = xgb.train(XGB_PARAMS, dtrain, num_boost_round=num_boost_round, evals=[(dtrain, 'train'), (dvalid, 'valid')], early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose_eval)\n",
    "\n",
    "    y_valid_pred = model.predict(dvalid)\n",
    "#     r2_score = calculate_r2_batched(y_valid.to_numpy(), y_valid_pred, w_valid.to_numpy())\n",
    "#     print(f\"Fold {fold_idx} validation R2 score: {r2_score}\")\n",
    "    print('\\nFold completed \\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test xgboost\n",
    "# import xgboost as xgb\n",
    "# import numpy as np\n",
    "\n",
    "# # Generate some random data to simulate a training set\n",
    "# num_samples = 1000\n",
    "# num_features = 10\n",
    "\n",
    "# X = np.random.rand(num_samples, num_features)\n",
    "# y = np.random.randint(2, size=num_samples)\n",
    "\n",
    "# # Split data into training and evaluation sets\n",
    "# split_index = int(num_samples * 0.8)\n",
    "# X_train, X_eval = X[:split_index], X[split_index:]\n",
    "# y_train, y_eval = y[:split_index], y[split_index:]\n",
    "\n",
    "# # Create DMatrix for train and eval data\n",
    "# dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "# deval = xgb.DMatrix(X_eval, label=y_eval)\n",
    "\n",
    "# # Define the parameter dictionary, enabling the use of GPU\n",
    "# param = {\n",
    "#     'objective': 'binary:logistic',\n",
    "#     'max_depth': 3,\n",
    "#     'eta': 0.1,\n",
    "#     'tree_method': 'hist',  # Specify 'gpu_hist' to enable GPU support\n",
    "#     'eval_metric': 'logloss',\n",
    "#     'device': 'cuda:4'\n",
    "# }\n",
    "\n",
    "# # Number of boosting rounds\n",
    "# num_round = 10\n",
    "\n",
    "# # Create a watchlist to evaluate the performance of the model\n",
    "# evals = [(dtrain, 'train'), (deval, 'eval')]\n",
    "\n",
    "# # Train the model\n",
    "# bst = xgb.train(param, dtrain, num_round, evals)\n",
    "\n",
    "# # Print model information\n",
    "# print(bst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "js",
   "language": "python",
   "name": "js"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
